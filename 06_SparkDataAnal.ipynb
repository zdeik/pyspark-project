{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aae0f57-500c-4adb-8d0d-f5722cef767e",
   "metadata": {},
   "source": [
    "#06_SparkDataAnal.ipynb\n",
    "TLC Trip Record Data\n",
    "출처: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55f9dbf9-77ee-477b-853d-d45ed544e8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"spark-sql\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aed4298-418f-4e9f-8ace-f2acbe6f4f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2015-summary.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0804289-68fd-4a6c-89f0-05a95ce1f183",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"json\")\\\n",
    "                .load(\"learning_spark_data/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54a11870-5076-4f4d-8502-3f3071771cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('DEST_COUNTRY_NAME', 'string'),\n",
       " ('ORIGIN_COUNTRY_NAME', 'string'),\n",
       " ('count', 'bigint')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa3504c6-bc95-4be8-99fc-f2e78f8cfe8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7d6a9a5-4762-4a78-ac7a-1bb9455f1fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f54bec20-d358-476f-a14e-2216e537682a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5b406a7-3e7d-4587-8a7a-de8e4fb20b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|   15|\n",
      "|    1|\n",
      "|  344|\n",
      "|   15|\n",
      "|   62|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('count').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36d531dc-0e7c-4753-b656-a5fb32eadc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "|    United States|\n",
      "|            Egypt|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('DEST_COUNTRY_NAME').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8c2d3d-defd-47b5-a8f2-17461f896bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('DEST_COUNTRY_NAME').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2334d024-37a6-4a7d-b0f4-a78233b0a9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 도시국가명 중복제거\n",
    "df1 = df.select('DEST_COUNTRY_NAME').distinct().cache()\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f767f457-35e8-478e-8553-0a3910d7a0c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Row('hello', None, 1, False)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ROW CLASS를 이용한 단일 레코드 생성\n",
    "\n",
    "from pyspark.sql import Row\n",
    "myRow = Row('hello', None, 1, False)\n",
    "myRow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5749d69-1f5e-46a7-b19a-259b167b2071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "|    United States|            Ireland|  344|        false|\n",
      "|            Egypt|      United States|   15|        false|\n",
      "|    United States|              India|   62|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 새로운 컬럼 추기하기\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df3 = df.withColumn('withinCountry', expr('ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME'))\n",
    "df3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08097d66-bcf0-4065-9c49-05765aa69f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.withColumn('withinCountry', expr('ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME'))\n",
    "df3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "42a0f662-2563-47aa-964f-c229de34338d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|withinCountry|\n",
      "+-----------------+-------------------+------+-------------+\n",
      "|    United States|      United States|370002|         true|\n",
      "+-----------------+-------------------+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.filter(df3.withinCountry == True).show()\n",
    "# df3.where('withinCountry == true').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d85c433-892c-4ce1-8332-d7f0a6285145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# case when count가 10이하이면 under, 10이상이면 upper로 변환 -> category 컬럼 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "99d1eb5e-7782-40d7-9930-0a2ad698a1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+--------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|category|\n",
      "+-----------------+-------------------+-----+-------------+--------+\n",
      "|    United States|            Romania|   15|        false|   upper|\n",
      "|    United States|            Croatia|    1|        false|   under|\n",
      "|    United States|            Ireland|  344|        false|   upper|\n",
      "|            Egypt|      United States|   15|        false|   upper|\n",
      "|    United States|              India|   62|        false|   upper|\n",
      "+-----------------+-------------------+-----+-------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = df3.withColumn(\n",
    "    'category', expr(\"CASE WHEN count <= 10 THEN 'under' ELSE 'upper' END\"))\n",
    "\n",
    "df4.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf4ed8a-4446-474a-8d80-da4abfdccbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe의 select(), where(), filter() 트렌스포메이션\n",
    "# show(), count() 액션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5e7db1c4-606d-4ba7-b7e1-9a7219c5da73",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc5f9b3-b9f1-4f01-b742-801ab5b8f919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 집계함수 dept,emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7528baec-b138-48d4-998d-324162c0476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df, dept_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d80a62e8-d505-49d3-a78a-1084316c8c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"spark-sql\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4ecdc900-b9e1-447d-a735-8cf4c825e692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+\n",
      "|deptno|     dname|     loc|\n",
      "+------+----------+--------+\n",
      "|    10|ACCOUNTING|NEW YORK|\n",
      "|    20|  RESEARCH|  DALLAS|\n",
      "|    30|     SALES| CHICAGO|\n",
      "|    40|OPERATIONS|  BOSTON|\n",
      "+------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"learning_spark_data/dept.csv\")\n",
    "\n",
    "dept_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "587ed4f9-d520-414c-b6aa-035d66333bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+--------+----+----------+----+----+------+\n",
      "|empno| ename|     job| mgr|  hiredate| sal|comm|deptno|\n",
      "+-----+------+--------+----+----------+----+----+------+\n",
      "| 7369| SMITH|   CLERK|7902|1980-12-17| 800|NULL|    20|\n",
      "| 7499| ALLEN|SALESMAN|7698|1981-02-20|1600| 300|    30|\n",
      "| 7521|  WARD|SALESMAN|7698|1981-02-22|1250| 500|    30|\n",
      "| 7566| JONES| MANAGER|7839|1981-04-02|2975|NULL|    20|\n",
      "| 7654|MARTIN|SALESMAN|7698|1981-09-28|1250|1400|    30|\n",
      "+-----+------+--------+----+----------+----+----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"learning_spark_data/emp.csv\")\n",
    "\n",
    "emp_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0898b8-b81d-4b99-94ec-f2ec84707610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter랑 동일함\n",
    "emp_df.select('*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bf082ed3-0cd9-4974-b5de-e963bdd58f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      15|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.selectExpr('count(*)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "46826198-63a9-4631-b955-1991872ce54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|count(DISTINCT job)|\n",
      "+-------------------+\n",
      "|                  5|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "emp_df.select(countDistinct('job')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ab4e2ad7-7295-4908-9446-c15febe557a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|approx_count_distinct(job)|\n",
      "+--------------------------+\n",
      "|                         5|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "emp_df.select(approx_count_distinct('job',0.1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289d14b5-c057-46e2-b048-eca3322a4ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, last, min, max, sum, avg ->expr:sql 문장 x , function으로 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "38bd8862-632e-4a70-8b40-165e5ef0121d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+--------+--------+------------------+\n",
      "|first(sal)|last(sal)|min(sal)|max(sal)|sum(sal)|          avg(sal)|\n",
      "+----------+---------+--------+--------+--------+------------------+\n",
      "|       800|     3200|     800|    5000|   32225|2148.3333333333335|\n",
      "+----------+---------+--------+--------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last, min, max, sum, avg\n",
    "\n",
    "emp_df.select(first('sal'),\n",
    "              last('sal'),\n",
    "               min('sal'),\n",
    "               max('sal'),\n",
    "               sum('sal'),\n",
    "              avg('sal')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "935bbe77-0b51-478e-82bc-2520461c3547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------+--------+--------+------------------+\n",
      "|first(sal)|last(sal)|min(sal)|max(sal)|sum(sal)|          avg(sal)|\n",
      "+----------+---------+--------+--------+--------+------------------+\n",
      "|       800|     3200|     800|    5000|   32225|2148.3333333333335|\n",
      "+----------+---------+--------+--------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# expr\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "emp_df.select(\n",
    "    expr(\"first(sal)\"),\n",
    "    expr(\"last(sal)\"),\n",
    "    expr(\"min(sal)\"),\n",
    "    expr(\"max(sal)\"),\n",
    "    expr(\"sum(sal)\"),\n",
    "    expr(\"avg(sal)\"),\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "789ecd5e-86a8-4285-a021-b3e5db41918d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|sum(DISTINCT sal)|\n",
      "+-----------------+\n",
      "|            27975|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.selectExpr('sum(DISTINCT sal)').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adae2d1-50bd-4580-b239-0b55116ed46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_salary, total_transaction, avg_salary, mean_sala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "af19601a-f6bb-417c-8354-8db09054727d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+------------------+------------------+\n",
      "|total_salary|total_transaction|        avg_salary|         mean_sala|\n",
      "+------------+-----------------+------------------+------------------+\n",
      "|       32225|             2200|2148.3333333333335|2148.3333333333335|\n",
      "+------------+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.selectExpr(\n",
    "    'sum(sal) as total_salary',\n",
    "    'sum(comm) as total_transaction',\n",
    "    'avg(sal) as avg_salary',\n",
    "    'avg(sal) as mean_sala'\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c0bcee44-5ef8-4b84-a822-c6e7bf58ae65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+----------+---------+\n",
      "|total_salary|total_transaction|avg_salary|mean_sala|\n",
      "+------------+-----------------+----------+---------+\n",
      "|       32225|             2200|   2148.33|  2148.33|\n",
      "+------------+-----------------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.selectExpr(\n",
    "    'sum(sal) as total_salary',\n",
    "    'sum(comm) as total_transaction',\n",
    "    'ROUND(avg(sal), 2) as avg_salary',\n",
    "    'ROUND(avg(sal), 2) as mean_sala'\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548755e6-5e00-40e1-810f-9df815380bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그룹화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e1793a7c-1a2d-470b-a7a3-7164eb47ed65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|      job|count|\n",
      "+---------+-----+\n",
      "|  ANALYST|    2|\n",
      "| SALESMAN|    4|\n",
      "|    CLERK|    5|\n",
      "|  MANAGER|    3|\n",
      "|PRESIDENT|    1|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.groupBy('job').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ce6369ff-3b43-4585-8f73-39e7bb6b420e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+------------+\n",
      "|      job|qty|total_salary|\n",
      "+---------+---+------------+\n",
      "|  ANALYST|  2|        6000|\n",
      "| SALESMAN|  4|        5600|\n",
      "|    CLERK|  5|        7350|\n",
      "|  MANAGER|  3|        8275|\n",
      "|PRESIDENT|  1|        5000|\n",
      "+---------+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select job,\n",
    "#     count(job),\n",
    "#     sum(sal)\n",
    "# group by job\n",
    "from pyspark.sql.functions import count, sum\n",
    "\n",
    "group_df = emp_df.groupby('job').agg(\n",
    "    count('job').alias('qty'),\n",
    "    sum('sal').alias('total_salary')\n",
    ")\n",
    "\n",
    "group_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "55563732-c93d-443d-99cf-d5e6c092cf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+---------+\n",
      "|      job|AVG_SAL|SAL_STDEV|\n",
      "+---------+-------+---------+\n",
      "|  ANALYST| 3000.0|      0.0|\n",
      "| SALESMAN| 1400.0|   177.95|\n",
      "|    CLERK| 1470.0|   984.63|\n",
      "|  MANAGER|2758.33|   274.24|\n",
      "|PRESIDENT| 5000.0|     NULL|\n",
      "+---------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sal의 평균 SAL,AVG, 표준편차 SAL_STDEV를 job별로 계산해서 출력, 소수점 2자리\n",
    "\n",
    "from pyspark.sql.functions import avg, stddev, round\n",
    "\n",
    "group_job_df = emp_df.groupBy('job').agg(\n",
    "    round(avg('sal'), 2).alias('AVG_SAL'),\n",
    "    round(stddev('sal'), 2).alias('SAL_STDEV')\n",
    ")\n",
    "\n",
    "group_job_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d72e26cc-1863-4449-9a6b-9487d00b5eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+\n",
      "|      job| sal|\n",
      "+---------+----+\n",
      "|PRESIDENT|5000|\n",
      "|    CLERK|3200|\n",
      "|  ANALYST|3000|\n",
      "|  ANALYST|3000|\n",
      "|  MANAGER|2975|\n",
      "|  MANAGER|2850|\n",
      "|  MANAGER|2450|\n",
      "| SALESMAN|1600|\n",
      "| SALESMAN|1500|\n",
      "|    CLERK|1300|\n",
      "+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "sal_top10_df = emp_df.select(\"job\", \"sal\") \\\n",
    "                     .orderBy(col(\"sal\").desc()) \\\n",
    "                     .limit(10)\n",
    "\n",
    "sal_top10_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "59c974e0-27f1-43b5-9a53-0f8f892be7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'RANK() OVER (ORDER BY sal DESC NULLS LAST unspecifiedframe$())'>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 윈도우 함수\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, desc\n",
    "windowspec = Window.orderBy(desc('sal'))\n",
    "salAllRank = rank().over(windowspec)\n",
    "salAllRank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f13c0719-5ed5-48a1-9cb8-ad25797bf2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+----+----+------+----------+\n",
      "|empno| ename|      job| mgr|  hiredate| sal|comm|deptno|slary_rank|\n",
      "+-----+------+---------+----+----------+----+----+------+----------+\n",
      "| 7839|  KING|PRESIDENT|NULL|1981-11-17|5000|NULL|    10|         1|\n",
      "| 9292|  JACK|    CLERK|7782|1982-01-23|3200|NULL|    70|         2|\n",
      "| 7788| SCOTT|  ANALYST|7566|1987-04-19|3000|NULL|    20|         3|\n",
      "| 7902|  FORD|  ANALYST|7566|1981-12-03|3000|NULL|    20|         3|\n",
      "| 7566| JONES|  MANAGER|7839|1981-04-02|2975|NULL|    20|         5|\n",
      "| 7698| BLAKE|  MANAGER|7839|1981-05-01|2850|NULL|    30|         6|\n",
      "| 7782| CLARK|  MANAGER|7839|1981-06-09|2450|NULL|    10|         7|\n",
      "| 7499| ALLEN| SALESMAN|7698|1981-02-20|1600| 300|    30|         8|\n",
      "| 7844|TURNER| SALESMAN|7698|1981-09-08|1500|   0|    30|         9|\n",
      "| 7934|MILLER|    CLERK|7782|1982-01-23|1300|NULL|    10|        10|\n",
      "| 7521|  WARD| SALESMAN|7698|1981-02-22|1250| 500|    30|        11|\n",
      "| 7654|MARTIN| SALESMAN|7698|1981-09-28|1250|1400|    30|        11|\n",
      "| 7876| ADAMS|    CLERK|7788|1987-05-23|1100|NULL|    20|        13|\n",
      "| 7900| JAMES|    CLERK|7698|1981-12-03| 950|NULL|    30|        14|\n",
      "| 7369| SMITH|    CLERK|7902|1980-12-17| 800|NULL|    20|        15|\n",
      "+-----+------+---------+----+----------+----+----+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.withColumn('slary_rank', salAllRank).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "088422dd-7fa0-4192-942f-70efb306468f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+-------------+\n",
      "|empno|deptno| sal|dept_sal_rank|\n",
      "+-----+------+----+-------------+\n",
      "| 7839|    10|5000|            1|\n",
      "| 7782|    10|2450|            2|\n",
      "| 7934|    10|1300|            3|\n",
      "| 7788|    20|3000|            1|\n",
      "| 7902|    20|3000|            1|\n",
      "| 7566|    20|2975|            3|\n",
      "| 7876|    20|1100|            4|\n",
      "| 7369|    20| 800|            5|\n",
      "| 7698|    30|2850|            1|\n",
      "| 7499|    30|1600|            2|\n",
      "| 7844|    30|1500|            3|\n",
      "| 7521|    30|1250|            4|\n",
      "| 7654|    30|1250|            4|\n",
      "| 7900|    30| 950|            6|\n",
      "| 9292|    70|3200|            1|\n",
      "+-----+------+----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#직무별로 rank작성\n",
    "#window, partitionBy\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, desc, col\n",
    "\n",
    "# 부서별로 나누고, 각 부서 내에서 급여 내림차순 정렬하여 순위 계산\n",
    "windowSpec = Window.partitionBy(\"deptno\").orderBy(desc(\"sal\"))\n",
    "\n",
    "ranked_df = emp_df.withColumn(\"dept_sal_rank\", rank().over(windowSpec))\n",
    "\n",
    "ranked_df.select(\"empno\", \"deptno\", \"sal\", \"dept_sal_rank\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "332bf3ca-c4d9-4a5f-ba72-ca31a01f80d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+----+----------------+\n",
      "|empno| ename|deptno| sal|dept_salary_rank|\n",
      "+-----+------+------+----+----------------+\n",
      "| 7839|  KING|    10|5000|               1|\n",
      "| 7782| CLARK|    10|2450|               2|\n",
      "| 7934|MILLER|    10|1300|               3|\n",
      "| 7788| SCOTT|    20|3000|               1|\n",
      "| 7902|  FORD|    20|3000|               1|\n",
      "| 7566| JONES|    20|2975|               3|\n",
      "| 7876| ADAMS|    20|1100|               4|\n",
      "| 7369| SMITH|    20| 800|               5|\n",
      "| 7698| BLAKE|    30|2850|               1|\n",
      "| 7499| ALLEN|    30|1600|               2|\n",
      "| 7844|TURNER|    30|1500|               3|\n",
      "| 7521|  WARD|    30|1250|               4|\n",
      "| 7654|MARTIN|    30|1250|               4|\n",
      "| 7900| JAMES|    30| 950|               6|\n",
      "| 9292|  JACK|    70|3200|               1|\n",
      "+-----+------+------+----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 부서별 순위\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, desc\n",
    "\n",
    "# 부서별로 파티션을 나누고, sal(급여) 내림차순으로 정렬\n",
    "windowSpec = Window.partitionBy(\"deptno\").orderBy(desc(\"sal\"))\n",
    "\n",
    "# 부서별 순위를 rank()로 계산\n",
    "ranked_df = emp_df.withColumn(\"dept_salary_rank\", rank().over(windowSpec))\n",
    "\n",
    "# 결과 출력\n",
    "ranked_df.select(\"empno\", \"ename\", \"deptno\", \"sal\", \"dept_salary_rank\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1775c9d6-861e-4574-bc23-2909bef32840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+----+----------+\n",
      "|empno| ename| sal|cum_salary|\n",
      "+-----+------+----+----------+\n",
      "| 7839|  KING|5000|      5000|\n",
      "| 9292|  JACK|3200|      8200|\n",
      "| 7788| SCOTT|3000|     14200|\n",
      "| 7902|  FORD|3000|     14200|\n",
      "| 7566| JONES|2975|     17175|\n",
      "| 7698| BLAKE|2850|     20025|\n",
      "| 7782| CLARK|2450|     22475|\n",
      "| 7499| ALLEN|1600|     24075|\n",
      "| 7844|TURNER|1500|     25575|\n",
      "| 7934|MILLER|1300|     26875|\n",
      "| 7521|  WARD|1250|     29375|\n",
      "| 7654|MARTIN|1250|     29375|\n",
      "| 7876| ADAMS|1100|     30475|\n",
      "| 7900| JAMES| 950|     31425|\n",
      "| 7369| SMITH| 800|     32225|\n",
      "+-----+------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 누적 급여 sum('sal').over()\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum, desc\n",
    "\n",
    "# 전체 데이터를 sal 기준으로 내림차순 정렬한 윈도우 스펙 정의\n",
    "windowSpec = Window.orderBy(desc('sal'))\n",
    "\n",
    "# 누적 급여 계산 (내림차순 누적합)\n",
    "emp_df.withColumn(\"cum_salary\", sum('sal').over(windowSpec)) \\\n",
    "      .select(\"empno\", \"ename\", \"sal\", \"cum_salary\") \\\n",
    "      .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e796845d-0ff1-45ce-9768-745cfbf588ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+----+---------------+\n",
      "|empno| ename|deptno| sal|dept_cum_salary|\n",
      "+-----+------+------+----+---------------+\n",
      "| 7839|  KING|    10|5000|           5000|\n",
      "| 7782| CLARK|    10|2450|           7450|\n",
      "| 7934|MILLER|    10|1300|           8750|\n",
      "| 7788| SCOTT|    20|3000|           6000|\n",
      "| 7902|  FORD|    20|3000|           6000|\n",
      "| 7566| JONES|    20|2975|           8975|\n",
      "| 7876| ADAMS|    20|1100|          10075|\n",
      "| 7369| SMITH|    20| 800|          10875|\n",
      "| 7698| BLAKE|    30|2850|           2850|\n",
      "| 7499| ALLEN|    30|1600|           4450|\n",
      "| 7844|TURNER|    30|1500|           5950|\n",
      "| 7521|  WARD|    30|1250|           8450|\n",
      "| 7654|MARTIN|    30|1250|           8450|\n",
      "| 7900| JAMES|    30| 950|           9400|\n",
      "| 9292|  JACK|    70|3200|           3200|\n",
      "+-----+------+------+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 부서별 누적급여\n",
    "windowSpec = Window.partitionBy(\"deptno\").orderBy(desc(\"sal\"))\n",
    "\n",
    "emp_df.withColumn(\"dept_cum_salary\", sum('sal').over(windowSpec)) \\\n",
    "      .select(\"empno\", \"ename\", \"deptno\", \"sal\", \"dept_cum_salary\") \\\n",
    "      .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3c246e3a-d60f-45b5-92e3-3fd3bd2100c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+----+------------------+--------------+\n",
      "|empno| ename|deptno| sal|      avg_dept_sal|compare_result|\n",
      "+-----+------+------+----+------------------+--------------+\n",
      "| 7782| CLARK|    10|2450|2916.6666666666665|     below_avg|\n",
      "| 7839|  KING|    10|5000|2916.6666666666665|     above_avg|\n",
      "| 7934|MILLER|    10|1300|2916.6666666666665|     below_avg|\n",
      "| 7369| SMITH|    20| 800|            2175.0|     below_avg|\n",
      "| 7566| JONES|    20|2975|            2175.0|     above_avg|\n",
      "| 7788| SCOTT|    20|3000|            2175.0|     above_avg|\n",
      "| 7876| ADAMS|    20|1100|            2175.0|     below_avg|\n",
      "| 7902|  FORD|    20|3000|            2175.0|     above_avg|\n",
      "| 7499| ALLEN|    30|1600|1566.6666666666667|     above_avg|\n",
      "| 7521|  WARD|    30|1250|1566.6666666666667|     below_avg|\n",
      "| 7654|MARTIN|    30|1250|1566.6666666666667|     below_avg|\n",
      "| 7698| BLAKE|    30|2850|1566.6666666666667|     above_avg|\n",
      "| 7844|TURNER|    30|1500|1566.6666666666667|     below_avg|\n",
      "| 7900| JAMES|    30| 950|1566.6666666666667|     below_avg|\n",
      "| 9292|  JACK|    70|3200|            3200.0|         equal|\n",
      "+-----+------+------+----+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 부서별 평균급여와 직원 개별 급여 비교\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg, when, col\n",
    "\n",
    "# 부서별 평균 급여 윈도우 정의\n",
    "windowSpec = Window.partitionBy('deptno')\n",
    "\n",
    "# 평균 급여 컬럼 추가 + 비교 결과 추가\n",
    "emp_df.withColumn('avg_dept_sal', avg('sal').over(windowSpec)) \\\n",
    "      .withColumn('compare_result',\n",
    "                  when(col('sal') > col('avg_dept_sal'), 'above_avg')\n",
    "                  .when(col('sal') < col('avg_dept_sal'), 'below_avg')\n",
    "                  .otherwise('equal')) \\\n",
    "      .select('empno', 'ename', 'deptno', 'sal', 'avg_dept_sal', 'compare_result') \\\n",
    "      .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29409300-0b2d-4091-9780-c5683143cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sql 조인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "626109ac-e49e-4704-9302-2f885ac60a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---------+----+----------+----+----+------+------+----------+--------+\n",
      "|empno| ename|      job| mgr|  hiredate| sal|comm|deptno|deptno|     dname|     loc|\n",
      "+-----+------+---------+----+----------+----+----+------+------+----------+--------+\n",
      "| 7369| SMITH|    CLERK|7902|1980-12-17| 800|NULL|    20|    20|  RESEARCH|  DALLAS|\n",
      "| 7499| ALLEN| SALESMAN|7698|1981-02-20|1600| 300|    30|    30|     SALES| CHICAGO|\n",
      "| 7521|  WARD| SALESMAN|7698|1981-02-22|1250| 500|    30|    30|     SALES| CHICAGO|\n",
      "| 7566| JONES|  MANAGER|7839|1981-04-02|2975|NULL|    20|    20|  RESEARCH|  DALLAS|\n",
      "| 7654|MARTIN| SALESMAN|7698|1981-09-28|1250|1400|    30|    30|     SALES| CHICAGO|\n",
      "| 7698| BLAKE|  MANAGER|7839|1981-05-01|2850|NULL|    30|    30|     SALES| CHICAGO|\n",
      "| 7782| CLARK|  MANAGER|7839|1981-06-09|2450|NULL|    10|    10|ACCOUNTING|NEW YORK|\n",
      "| 7788| SCOTT|  ANALYST|7566|1987-04-19|3000|NULL|    20|    20|  RESEARCH|  DALLAS|\n",
      "| 7839|  KING|PRESIDENT|NULL|1981-11-17|5000|NULL|    10|    10|ACCOUNTING|NEW YORK|\n",
      "| 7844|TURNER| SALESMAN|7698|1981-09-08|1500|   0|    30|    30|     SALES| CHICAGO|\n",
      "| 7876| ADAMS|    CLERK|7788|1987-05-23|1100|NULL|    20|    20|  RESEARCH|  DALLAS|\n",
      "| 7900| JAMES|    CLERK|7698|1981-12-03| 950|NULL|    30|    30|     SALES| CHICAGO|\n",
      "| 7902|  FORD|  ANALYST|7566|1981-12-03|3000|NULL|    20|    20|  RESEARCH|  DALLAS|\n",
      "| 7934|MILLER|    CLERK|7782|1982-01-23|1300|NULL|    10|    10|ACCOUNTING|NEW YORK|\n",
      "+-----+------+---------+----+----------+----+----+------+------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_dept_df = emp_df.join(dept_df, emp_df['deptno']==dept_df['deptno'])\n",
    "emp_dept_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5596c750-2ef5-407f-8d37-1c4c6ddc8e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "join_df = emp_df.join(dept_df, on='deptno', how='inner')\n",
    "join_df.select('ename','','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e5c27f94-0e8f-4bd6-9ebf-992d23324733",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3742d6ee-3f15-491f-90cd-a9fc8ad1269d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aab8c57-6252-41e1-9084-e0bcb0542b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad3c43e-75ba-4b5d-a554-f02cb4255e05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ea7d79-58c7-4e30-b14b-597380229514",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c5410a-9389-433a-a05e-3793d9ad112e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
